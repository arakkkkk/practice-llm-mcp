# LLMでMCPサーバーを試してみる

## LLLM

### Tools
* Ollama
* Open WebUI

### Model

| モデル名                | パラメータ規模           | 特徴・性能のポイント                                                                     | 推奨される使い方                                                                         |
| ----------------------- | ------------------------ | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **phi3\:mini**          | 2.7B                     | ・非常に軽量<br>・パフォーマンスのコスパが高い<br>・小型モデルとしては驚異的な知能を持つ | ・軽量環境での高速推論<br>・簡単な雑談や補助的タスク<br>・低リソース端末向け             |
| **llama3:8b**           | 8B                       | ・Meta製<br>・GPT-3.5相当の精度<br>・速度と精度のバランス良し                            | ・汎用対話<br>・文章生成・要約など幅広い用途<br>・リソースに余裕がある環境               |
| **qwen:7b-chat**        | 7B                       | ・7Bで軽量<br>・14B版より小さい分、速度向上<br>・チャット用に特化調整済み                | ・対話・チャット用途に最適<br>・高速応答が求められるシステム<br>・軽量ながら質の高い会話 |
| **mistral**             | 約7B(未公開だが7B台推定) | ・超高速推論が特徴<br>・Q\&Aや雑談に強い<br>・高効率なトークン生成                       | ・リアルタイム対話<br>・FAQやカスタマーサポートチャット<br>・即時性重視の応答            |
| **deepseek-coder:6.7b** | 6.7B                     | ・コード生成に特化したモデル<br>・プログラミング言語の理解・生成が優秀                   | ・コード生成・補完<br>・プログラマーのアシスタント<br>・技術文書作成やレビュー支援       |

## MCP
