# LLMでMCPサーバーを試してみる

## 参考
[Dockerを使ってOllamaとOpen WebUIでllama3を動かす](https://zenn.dev/misora/articles/1037a94c53a5f0)

## 使い方

### 準備: コンテナ作成

```sh
$ sudo docker compose up -d
```

### 利用: ブラウザから

[http://localhost:8080](http://localhost:8080)へアクセス

### 利用: APIから

```sh
curl http://localhost:11434/api/chat -d '{
  "model": "phi3:mini",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ],
  "stream": false
}'
```

## LLLM

### Tools
* Ollama
* Open WebUI

### Model

| モデル名         | パラメータ規模           | 特徴・性能のポイント                                                                     | 推奨される使い方                                                                         |
| ---------------- | ------------------------ | ---------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| **phi3\:mini**   | 2.7B                     | ・非常に軽量<br>・パフォーマンスのコスパが高い<br>・小型モデルとしては驚異的な知能を持つ | ・軽量環境での高速推論<br>・簡単な雑談や補助的タスク<br>・低リソース端末向け             |
| **llama3:8b**    | 8B                       | ・Meta製<br>・GPT-3.5相当の精度<br>・速度と精度のバランス良し                            | ・汎用対話<br>・文章生成・要約など幅広い用途<br>・リソースに余裕がある環境               |
| **qwen:7b-chat** | 7B                       | ・7Bで軽量<br>・14B版より小さい分、速度向上<br>・チャット用に特化調整済み                | ・対話・チャット用途に最適<br>・高速応答が求められるシステム<br>・軽量ながら質の高い会話 |
| **mistral**      | 約7B(未公開だが7B台推定) | ・超高速推論が特徴<br>・Q\&Aや雑談に強い<br>・高効率なトークン生成                       | ・リアルタイム対話<br>・FAQやカスタマーサポートチャット<br>・即時性重視の応答            |

## MCP
