#!/bin/bash

echo "ğŸ”§ Starting model initialization: qwen"

# Wait until the Ollama server is fully up and running
until bash -c '</dev/tcp/localhost/11434' 2>/dev/null; do
  echo "â³ Waiting for Ollama to start..."
  sleep 5
done

# Pull the model
ollama pull phi3:mini # éå¸¸ã«è»½é‡ï¼ˆ2.7Bï¼‰ã§ã‚‚è³¢ã„ï¼ˆé©šç•°çš„ï¼‰
ollama pull llama3:8b # ç²¾åº¦ã‚‚é€Ÿåº¦ã‚‚ãƒãƒ©ãƒ³ã‚¹è‰¯ã—ï¼ˆMetaè£½ï¼‰ GPT-3.5ãƒ¬ãƒ™ãƒ«
ollama pull qwen:7b-chat # 14b ã‚ˆã‚Šè»½ãã€å¯¾è©±èª¿æ•´ã•ã‚Œã¦ã„ã¦ä½¿ã„ã‚„ã™ã„
ollama pull mistral # è¶…é«˜é€Ÿã€Q&Aãƒ»é›‘è«‡ã«å¼·ã„
ollama deepseek-coder:6.7b # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«

echo "âœ… Model is ready"
